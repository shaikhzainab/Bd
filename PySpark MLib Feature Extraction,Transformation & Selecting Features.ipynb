{"cells":[{"cell_type":"code","source":["#TF-IDF(Term frequency-inverse document frequency)\n#TF: Both HashingTF and CountVectorizer can be used to generate the term frequency vectors.\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Feature Extractors","showTitle":true,"inputWidgets":{},"nuid":"0f561c7f-b637-454d-a732-6cb7623a619e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sentenceData = spark.createDataFrame([\n    (0.0, \"Hi I heard about Spark\"),\n    (0.0, \"I wish Java could use case classes\"),\n    (1.0, \"Logistic regression models are neat\")\n], [\"label\", \"sentence\"])\n\ntokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\nwordsData = tokenizer.transform(sentenceData)\n\nhashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\nfeaturizedData = hashingTF.transform(wordsData)\n# alternatively, CountVectorizer can also be used to get term frequency vectors\n\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\nidfModel = idf.fit(featurizedData)\nrescaledData = idfModel.transform(featurizedData)\n\nrescaledData.select(\"label\", \"features\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7628e987-2bea-4ccd-a09b-faf66eed9f2d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+--------------------+\n|label|            features|\n+-----+--------------------+\n|  0.0|(20,[6,8,13,16],[...|\n|  0.0|(20,[0,2,7,13,15,...|\n|  1.0|(20,[3,4,6,11,19]...|\n+-----+--------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+--------------------+\n|label|            features|\n+-----+--------------------+\n|  0.0|(20,[6,8,13,16],[...|\n|  0.0|(20,[0,2,7,13,15,...|\n|  1.0|(20,[3,4,6,11,19]...|\n+-----+--------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import Word2Vec\n\n# Input data: Each row is a bag of words from a sentence or document.\ndocumentDF = spark.createDataFrame([\n    (\"Hi I heard about Spark\".split(\" \"), ),\n    (\"I wish Java could use case classes\".split(\" \"), ),\n    (\"Logistic regression models are neat\".split(\" \"), )\n], [\"text\"])\n\n# Learn a mapping from words to Vectors.\nword2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\nmodel = word2Vec.fit(documentDF)\n\nresult = model.transform(documentDF)\nfor row in result.collect():\n    text, vector = row\n    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Word2Vec","showTitle":true,"inputWidgets":{},"nuid":"fa243ed9-d776-4d0d-9735-9e5deb9a8f7d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Text: [Hi, I, heard, about, Spark] => \nVector: [0.012264367192983627,-0.06442034244537354,-0.007622340321540833]\n\nText: [I, wish, Java, could, use, case, classes] => \nVector: [0.05160687722465289,0.025969027541577816,0.02736483487699713]\n\nText: [Logistic, regression, models, are, neat] => \nVector: [-0.06564115285873413,0.02060299552977085,-0.08455150425434113]\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Text: [Hi, I, heard, about, Spark] => \nVector: [0.012264367192983627,-0.06442034244537354,-0.007622340321540833]\n\nText: [I, wish, Java, could, use, case, classes] => \nVector: [0.05160687722465289,0.025969027541577816,0.02736483487699713]\n\nText: [Logistic, regression, models, are, neat] => \nVector: [-0.06564115285873413,0.02060299552977085,-0.08455150425434113]\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import CountVectorizer\n\n# Input data: Each row is a bag of words with a ID.\ndf = spark.createDataFrame([\n    (0, \"a b c\".split(\" \")),\n    (1, \"a b b c a\".split(\" \"))\n], [\"id\", \"words\"])\n\n# fit a CountVectorizerModel from the corpus.\ncv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n\nmodel = cv.fit(df)\n\nresult = model.transform(df)\nresult.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"CountVectorizer","showTitle":true,"inputWidgets":{},"nuid":"64ab6444-d1ed-451b-9bde-8ac6ad696d7d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+---------------+-------------------------+\n|id |words          |features                 |\n+---+---------------+-------------------------+\n|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+---+---------------+-------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+---------------+-------------------------+\n|id |words          |features                 |\n+---+---------------+-------------------------+\n|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+---+---------------+-------------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Feature Transformers","showTitle":true,"inputWidgets":{},"nuid":"78240d57-d4de-45a5-a134-65995e0f1464"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import Tokenizer, RegexTokenizer\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import IntegerType\n\nsentenceDataFrame = spark.createDataFrame([\n    (0, \"Hi I heard about Spark\"),\n    (1, \"I wish Java could use case classes\"),\n    (2, \"Logistic,regression,models,are,neat\")\n], [\"id\", \"sentence\"])\n\ntokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n\nregexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n# alternatively, pattern=\"\\\\w+\", gaps(False)\n\ncountTokens = udf(lambda words: len(words), IntegerType())\n\ntokenized = tokenizer.transform(sentenceDataFrame)\ntokenized.select(\"sentence\", \"words\")\\\n    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n\nregexTokenized = regexTokenizer.transform(sentenceDataFrame)\nregexTokenized.select(\"sentence\", \"words\") \\\n    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Tokenizer","showTitle":true,"inputWidgets":{},"nuid":"26feb813-7810-4228-84bd-7757a5d71991"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------------------------+------------------------------------------+------+\n|sentence                           |words                                     |tokens|\n+-----------------------------------+------------------------------------------+------+\n|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n+-----------------------------------+------------------------------------------+------+\n\n+-----------------------------------+------------------------------------------+------+\n|sentence                           |words                                     |tokens|\n+-----------------------------------+------------------------------------------+------+\n|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n+-----------------------------------+------------------------------------------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------------------------+------------------------------------------+------+\n|sentence                           |words                                     |tokens|\n+-----------------------------------+------------------------------------------+------+\n|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n+-----------------------------------+------------------------------------------+------+\n\n+-----------------------------------+------------------------------------------+------+\n|sentence                           |words                                     |tokens|\n+-----------------------------------+------------------------------------------+------+\n|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n+-----------------------------------+------------------------------------------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import StopWordsRemover\n\nsentenceData = spark.createDataFrame([\n    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n], [\"id\", \"raw\"])\n\nremover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\nremover.transform(sentenceData).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"StopWordsRemover","showTitle":true,"inputWidgets":{},"nuid":"d64563a0-ed3d-466d-a4c1-0d50133c9a95"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+----------------------------+--------------------+\n|id |raw                         |filtered            |\n+---+----------------------------+--------------------+\n|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n+---+----------------------------+--------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----------------------------+--------------------+\n|id |raw                         |filtered            |\n+---+----------------------------+--------------------+\n|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n+---+----------------------------+--------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import NGram\n\nwordDataFrame = spark.createDataFrame([\n    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n], [\"id\", \"words\"])\n\nngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n\nngramDataFrame = ngram.transform(wordDataFrame)\nngramDataFrame.select(\"ngrams\").show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"n -gram","showTitle":true,"inputWidgets":{},"nuid":"add251be-17c0-4966-af8e-c37fb9d69a1e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------------------------------------------------------------------+\n|ngrams                                                            |\n+------------------------------------------------------------------+\n|[Hi I, I heard, heard about, about Spark]                         |\n|[I wish, wish Java, Java could, could use, use case, case classes]|\n|[Logistic regression, regression models, models are, are neat]    |\n+------------------------------------------------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------------------------------------------------------------+\n|ngrams                                                            |\n+------------------------------------------------------------------+\n|[Hi I, I heard, heard about, about Spark]                         |\n|[I wish, wish Java, Java could, could use, use case, case classes]|\n|[Logistic regression, regression models, models are, are neat]    |\n+------------------------------------------------------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import Binarizer\n\ncontinuousDataFrame = spark.createDataFrame([\n    (0, 0.1),\n    (1, 0.8),\n    (2, 0.2)\n], [\"id\", \"feature\"])\n\nbinarizer = Binarizer(threshold=0.5, inputCol=\"feature\", outputCol=\"binarized_feature\")\n\nbinarizedDataFrame = binarizer.transform(continuousDataFrame)\n\nprint(\"Binarizer output with Threshold = %f\" % binarizer.getThreshold())\nbinarizedDataFrame.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Binarizer","showTitle":true,"inputWidgets":{},"nuid":"389ecfe8-0f4e-4461-afdd-db1f7088306c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Binarizer output with Threshold = 0.500000\n+---+-------+-----------------+\n| id|feature|binarized_feature|\n+---+-------+-----------------+\n|  0|    0.1|              0.0|\n|  1|    0.8|              1.0|\n|  2|    0.2|              0.0|\n+---+-------+-----------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Binarizer output with Threshold = 0.500000\n+---+-------+-----------------+\n| id|feature|binarized_feature|\n+---+-------+-----------------+\n|  0|    0.1|              0.0|\n|  1|    0.8|              1.0|\n|  2|    0.2|              0.0|\n+---+-------+-----------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import PCA\nfrom pyspark.ml.linalg import Vectors\n\ndata = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\ndf = spark.createDataFrame(data, [\"features\"])\n\npca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\nmodel = pca.fit(df)\n\nresult = model.transform(df).select(\"pcaFeatures\")\nresult.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"PCA","showTitle":true,"inputWidgets":{},"nuid":"6e91ecd8-d840-4d84-9a0a-7f50d8e23d36"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------------------------------------------------------------+\n|pcaFeatures                                                 |\n+------------------------------------------------------------+\n|[1.6485728230883814,-4.0132827005162985,-1.0091435193998504]|\n|[-4.645104331781533,-1.1167972663619048,-1.0091435193998501]|\n|[-6.428880535676488,-5.337951427775359,-1.009143519399851]  |\n+------------------------------------------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------------------------------------------------------+\n|pcaFeatures                                                 |\n+------------------------------------------------------------+\n|[1.6485728230883814,-4.0132827005162985,-1.0091435193998504]|\n|[-4.645104331781533,-1.1167972663619048,-1.0091435193998501]|\n|[-6.428880535676488,-5.337951427775359,-1.009143519399851]  |\n+------------------------------------------------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import PolynomialExpansion\nfrom pyspark.ml.linalg import Vectors\n\ndf = spark.createDataFrame([\n    (Vectors.dense([2.0, 1.0]),),\n    (Vectors.dense([0.0, 0.0]),),\n    (Vectors.dense([3.0, -1.0]),)\n], [\"features\"])\n\npolyExpansion = PolynomialExpansion(degree=3, inputCol=\"features\", outputCol=\"polyFeatures\")\npolyDF = polyExpansion.transform(df)\n\npolyDF.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"PolynomialExpansion","showTitle":true,"inputWidgets":{},"nuid":"06708cc2-69dc-4248-95e1-53638a03fb45"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+------------------------------------------+\n|features  |polyFeatures                              |\n+----------+------------------------------------------+\n|[2.0,1.0] |[2.0,4.0,8.0,1.0,2.0,4.0,1.0,2.0,1.0]     |\n|[0.0,0.0] |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]     |\n|[3.0,-1.0]|[3.0,9.0,27.0,-1.0,-3.0,-9.0,1.0,3.0,-1.0]|\n+----------+------------------------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+------------------------------------------+\n|features  |polyFeatures                              |\n+----------+------------------------------------------+\n|[2.0,1.0] |[2.0,4.0,8.0,1.0,2.0,4.0,1.0,2.0,1.0]     |\n|[0.0,0.0] |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]     |\n|[3.0,-1.0]|[3.0,9.0,27.0,-1.0,-3.0,-9.0,1.0,3.0,-1.0]|\n+----------+------------------------------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder\n\ndf = spark.createDataFrame([\n    (0.0, 1.0),\n    (1.0, 0.0),\n    (2.0, 1.0),\n    (0.0, 2.0),\n    (0.0, 1.0),\n    (2.0, 0.0)\n], [\"categoryIndex1\", \"categoryIndex2\"])\n\nencoder = OneHotEncoder(inputCols=[\"categoryIndex1\", \"categoryIndex2\"],\n                        outputCols=[\"categoryVec1\", \"categoryVec2\"])\nmodel = encoder.fit(df)\nencoded = model.transform(df)\nencoded.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"OneHotEncoder","showTitle":true,"inputWidgets":{},"nuid":"7efb74ee-0ef3-4ef8-a709-328614ca8c49"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------+--------------+-------------+-------------+\n|categoryIndex1|categoryIndex2| categoryVec1| categoryVec2|\n+--------------+--------------+-------------+-------------+\n|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n|           1.0|           0.0|(2,[1],[1.0])|(2,[0],[1.0])|\n|           2.0|           1.0|    (2,[],[])|(2,[1],[1.0])|\n|           0.0|           2.0|(2,[0],[1.0])|    (2,[],[])|\n|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n|           2.0|           0.0|    (2,[],[])|(2,[0],[1.0])|\n+--------------+--------------+-------------+-------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------+--------------+-------------+-------------+\n|categoryIndex1|categoryIndex2| categoryVec1| categoryVec2|\n+--------------+--------------+-------------+-------------+\n|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n|           1.0|           0.0|(2,[1],[1.0])|(2,[0],[1.0])|\n|           2.0|           1.0|    (2,[],[])|(2,[1],[1.0])|\n|           0.0|           2.0|(2,[0],[1.0])|    (2,[],[])|\n|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n|           2.0|           0.0|    (2,[],[])|(2,[0],[1.0])|\n+--------------+--------------+-------------+-------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import Normalizer\nfrom pyspark.ml.linalg import Vectors\n\ndataFrame = spark.createDataFrame([\n    (0, Vectors.dense([1.0, 0.5, -1.0]),),\n    (1, Vectors.dense([2.0, 1.0, 1.0]),),\n    (2, Vectors.dense([4.0, 10.0, 2.0]),)\n], [\"id\", \"features\"])\n\n# Normalize each Vector using $L^1$ norm.\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\nl1NormData = normalizer.transform(dataFrame)\nprint(\"Normalized using L^1 norm\")\nl1NormData.show()\n\n# Normalize each Vector using $L^\\infty$ norm.\nlInfNormData = normalizer.transform(dataFrame, {normalizer.p: float(\"inf\")})\nprint(\"Normalized using L^inf norm\")\nlInfNormData.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Normalizer","showTitle":true,"inputWidgets":{},"nuid":"2338cfdc-8660-4ee0-8b3e-edf7b22dc033"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Normalized using L^1 norm\n+---+--------------+------------------+\n| id|      features|      normFeatures|\n+---+--------------+------------------+\n|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|\n|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|\n|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|\n+---+--------------+------------------+\n\nNormalized using L^inf norm\n+---+--------------+--------------+\n| id|      features|  normFeatures|\n+---+--------------+--------------+\n|  0|[1.0,0.5,-1.0]|[1.0,0.5,-1.0]|\n|  1| [2.0,1.0,1.0]| [1.0,0.5,0.5]|\n|  2|[4.0,10.0,2.0]| [0.4,1.0,0.2]|\n+---+--------------+--------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Normalized using L^1 norm\n+---+--------------+------------------+\n| id|      features|      normFeatures|\n+---+--------------+------------------+\n|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|\n|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|\n|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|\n+---+--------------+------------------+\n\nNormalized using L^inf norm\n+---+--------------+--------------+\n| id|      features|  normFeatures|\n+---+--------------+--------------+\n|  0|[1.0,0.5,-1.0]|[1.0,0.5,-1.0]|\n|  1| [2.0,1.0,1.0]| [1.0,0.5,0.5]|\n|  2|[4.0,10.0,2.0]| [0.4,1.0,0.2]|\n+---+--------------+--------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Feature Selectors ","showTitle":true,"inputWidgets":{},"nuid":"3d993217-626e-40ee-9859-5755f98c3b64"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import ChiSqSelector\nfrom pyspark.ml.linalg import Vectors\n\ndf = spark.createDataFrame([\n    (7, Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0,),\n    (8, Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0,),\n    (9, Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0,)], [\"id\", \"features\", \"clicked\"])\n\nselector = ChiSqSelector(numTopFeatures=1, featuresCol=\"features\",\n                         outputCol=\"selectedFeatures\", labelCol=\"clicked\")\n\nresult = selector.fit(df).transform(df)\n\nprint(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\nresult.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"ChiSqSelector","showTitle":true,"inputWidgets":{},"nuid":"e70983f8-3f43-465b-ab49-24c5a906fdf4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"ChiSqSelector output with top 1 features selected\n+---+------------------+-------+----------------+\n| id|          features|clicked|selectedFeatures|\n+---+------------------+-------+----------------+\n|  7|[0.0,0.0,18.0,1.0]|    1.0|          [18.0]|\n|  8|[0.0,1.0,12.0,0.0]|    0.0|          [12.0]|\n|  9|[1.0,0.0,15.0,0.1]|    0.0|          [15.0]|\n+---+------------------+-------+----------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["ChiSqSelector output with top 1 features selected\n+---+------------------+-------+----------------+\n| id|          features|clicked|selectedFeatures|\n+---+------------------+-------+----------------+\n|  7|[0.0,0.0,18.0,1.0]|    1.0|          [18.0]|\n|  8|[0.0,1.0,12.0,0.0]|    0.0|          [12.0]|\n|  9|[1.0,0.0,15.0,0.1]|    0.0|          [15.0]|\n+---+------------------+-------+----------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import UnivariateFeatureSelector\nfrom pyspark.ml.linalg import Vectors\n\ndf = spark.createDataFrame([\n    (1, Vectors.dense([1.7, 4.4, 7.6, 5.8, 9.6, 2.3]), 3.0,),\n    (2, Vectors.dense([8.8, 7.3, 5.7, 7.3, 2.2, 4.1]), 2.0,),\n    (3, Vectors.dense([1.2, 9.5, 2.5, 3.1, 8.7, 2.5]), 3.0,),\n    (4, Vectors.dense([3.7, 9.2, 6.1, 4.1, 7.5, 3.8]), 2.0,),\n    (5, Vectors.dense([8.9, 5.2, 7.8, 8.3, 5.2, 3.0]), 4.0,),\n    (6, Vectors.dense([7.9, 8.5, 9.2, 4.0, 9.4, 2.1]), 4.0,)], [\"id\", \"features\", \"label\"])\n\nselector = UnivariateFeatureSelector(featuresCol=\"features\", outputCol=\"selectedFeatures\",\n                                     labelCol=\"label\", selectionMode=\"numTopFeatures\")\nselector.setFeatureType(\"continuous\").setLabelType(\"categorical\").setSelectionThreshold(1)\n\nresult = selector.fit(df).transform(df)\n\nprint(\"UnivariateFeatureSelector output with top %d features selected using f_classif\"\n      % selector.getSelectionThreshold())\nresult.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"UnivariateFeatureSelector","showTitle":true,"inputWidgets":{},"nuid":"42372cfe-65e6-4642-9b8a-9c6d6757a5b7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"UnivariateFeatureSelector output with top 1 features selected using f_classif\n+---+--------------------+-----+----------------+\n| id|            features|label|selectedFeatures|\n+---+--------------------+-----+----------------+\n|  1|[1.7,4.4,7.6,5.8,...|  3.0|           [2.3]|\n|  2|[8.8,7.3,5.7,7.3,...|  2.0|           [4.1]|\n|  3|[1.2,9.5,2.5,3.1,...|  3.0|           [2.5]|\n|  4|[3.7,9.2,6.1,4.1,...|  2.0|           [3.8]|\n|  5|[8.9,5.2,7.8,8.3,...|  4.0|           [3.0]|\n|  6|[7.9,8.5,9.2,4.0,...|  4.0|           [2.1]|\n+---+--------------------+-----+----------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["UnivariateFeatureSelector output with top 1 features selected using f_classif\n+---+--------------------+-----+----------------+\n| id|            features|label|selectedFeatures|\n+---+--------------------+-----+----------------+\n|  1|[1.7,4.4,7.6,5.8,...|  3.0|           [2.3]|\n|  2|[8.8,7.3,5.7,7.3,...|  2.0|           [4.1]|\n|  3|[1.2,9.5,2.5,3.1,...|  3.0|           [2.5]|\n|  4|[3.7,9.2,6.1,4.1,...|  2.0|           [3.8]|\n|  5|[8.9,5.2,7.8,8.3,...|  4.0|           [3.0]|\n|  6|[7.9,8.5,9.2,4.0,...|  4.0|           [2.1]|\n+---+--------------------+-----+----------------+\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark MLib:Feature Extraction,Transformation & Selecting Features","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2682825334014532}},"nbformat":4,"nbformat_minor":0}
