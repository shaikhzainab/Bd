{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"DataFrame Creation","showTitle":true,"inputWidgets":{},"nuid":"bd6b65e7-e4fd-4eac-9f25-4799398e9ac9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from datetime import datetime, date\nimport pandas as pd\nfrom pyspark.sql import Row\n\ndf = spark.createDataFrame([\n    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0)) ])\ndf\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b05db52a-260b-4818-85f9-ffe02ba0bf97"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[4]: DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[4]: DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.createDataFrame([\n    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))], \n    schema='a long, b double, c string, d date, e timestamp')\ndf\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Create a PySpark DataFrame with an explicit schema.","showTitle":true,"inputWidgets":{},"nuid":"63790aa9-15e6-493f-a2dc-63e15114e24e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[5]: DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[5]: DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"]}}],"execution_count":0},{"cell_type":"code","source":["pandas_df = pd.DataFrame({\n    'a': [1, 2, 3],\n    'b': [2., 3., 4.],\n    'c': ['string1', 'string2', 'string3'],\n    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n})\n\ndf = spark.createDataFrame(pandas_df)\ndf\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":" Create a PySpark DataFrame from a pandas DataFrame.","showTitle":true,"inputWidgets":{},"nuid":"9c11466e-6709-4a25-b0c8-442cd192a91f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[6]: DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[6]: DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"]}}],"execution_count":0},{"cell_type":"code","source":["rdd = spark.sparkContext.parallelize([\n    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n])\n\ndf = spark.createDataFrame(rdd, schema=['a', 'b', 'c', 'd', 'e'])\ndf\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":" Create a PySpark DataFrame from an RDD consisting of a list of tuples.","showTitle":true,"inputWidgets":{},"nuid":"7df208a5-bdf3-4967-8e1a-bfb14d884d26"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[7]: DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[7]: DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"]}}],"execution_count":0},{"cell_type":"code","source":["#All DataFrames above result the same.\ndf.show()\ndf.printSchema()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5b13602-e31e-4048-85f5-57ddd015b33c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+---+-------+----------+-------------------+\n|  a|  b|      c|         d|                  e|\n+---+---+-------+----------+-------------------+\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n+---+---+-------+----------+-------------------+\n\nroot\n |-- a: long (nullable = true)\n |-- b: double (nullable = true)\n |-- c: string (nullable = true)\n |-- d: date (nullable = true)\n |-- e: timestamp (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+---+-------+----------+-------------------+\n|  a|  b|      c|         d|                  e|\n+---+---+-------+----------+-------------------+\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n+---+---+-------+----------+-------------------+\n\nroot\n |-- a: long (nullable = true)\n |-- b: double (nullable = true)\n |-- c: string (nullable = true)\n |-- d: date (nullable = true)\n |-- e: timestamp (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":[" #The top rows of a DataFrame by using DataFrame.show().\ndf.show(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":" Viewing Data","showTitle":true,"inputWidgets":{},"nuid":"4f44856c-2258-45a7-8a54-7547e16e7bd9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+---+-------+----------+-------------------+\n|  a|  b|      c|         d|                  e|\n+---+---+-------+----------+-------------------+\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n+---+---+-------+----------+-------------------+\nonly showing top 1 row\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+---+-------+----------+-------------------+\n|  a|  b|      c|         d|                  e|\n+---+---+-------+----------+-------------------+\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n+---+---+-------+----------+-------------------+\nonly showing top 1 row\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#you can enable spark.sql.repl.eagerEval.enabled configuration for the eager evaluation of PySpark DataFrame in notebooks such as Jupyter. \n#The number of rows to show can be controlled via spark.sql.repl.eagerEval.maxNumRows configuration.\n\nspark.conf.set('spark.sql.repl.eagerEval.enabled', True)\ndf\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7bafd55e-aa90-4865-ba89-a6fcbff0fd7f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<table border='1'>\n<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n<tr><td>3</td><td>4.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n</table>\n","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<table border='1'>\n<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n<tr><td>3</td><td>4.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n</table>\n"]}}],"execution_count":0},{"cell_type":"code","source":[" #This is useful when rows are too long to show horizontally.\ndf.show(1, vertical=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":" rows in vertical","showTitle":true,"inputWidgets":{},"nuid":"e3bb9767-58eb-461a-9d5e-98953820b261"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"-RECORD 0------------------\n a   | 1                   \n b   | 2.0                 \n c   | string1             \n d   | 2000-01-01          \n e   | 2000-01-01 12:00:00 \nonly showing top 1 row\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["-RECORD 0------------------\n a   | 1                   \n b   | 2.0                 \n c   | string1             \n d   | 2000-01-01          \n e   | 2000-01-01 12:00:00 \nonly showing top 1 row\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df.columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"DataFrameâ€™s schema and column names ","showTitle":true,"inputWidgets":{},"nuid":"c83a4058-b3e4-48d9-8e4f-8ee6f681aad8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[12]: ['a', 'b', 'c', 'd', 'e']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[12]: ['a', 'b', 'c', 'd', 'e']"]}}],"execution_count":0},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"238ebcaf-1177-43a6-8256-9a565afaa58d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- a: long (nullable = true)\n |-- b: double (nullable = true)\n |-- c: string (nullable = true)\n |-- d: date (nullable = true)\n |-- e: timestamp (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- a: long (nullable = true)\n |-- b: double (nullable = true)\n |-- c: string (nullable = true)\n |-- d: date (nullable = true)\n |-- e: timestamp (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df.select(\"a\", \"b\", \"c\").describe().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"summary of the DataFrame.","showTitle":true,"inputWidgets":{},"nuid":"84c93243-beba-4e36-a62d-94df959b28b3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+---+---+-------+\n|summary|  a|  b|      c|\n+-------+---+---+-------+\n|  count|  3|  3|      3|\n|   mean|2.0|3.0|   null|\n| stddev|1.0|1.0|   null|\n|    min|  1|2.0|string1|\n|    max|  3|4.0|string3|\n+-------+---+---+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+---+---+-------+\n|summary|  a|  b|      c|\n+-------+---+---+-------+\n|  count|  3|  3|      3|\n|   mean|2.0|3.0|   null|\n| stddev|1.0|1.0|   null|\n|    min|  1|2.0|string1|\n|    max|  3|4.0|string3|\n+-------+---+---+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#DataFrame.collect() collects the distributed data to the driver side as the local data in Python. \n#this can throw an out-of-memory error when the dataset is too large to fit in the driver side because it collects all the data from executors to the driver side.\ndf.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"  collect","showTitle":true,"inputWidgets":{},"nuid":"65a9f760-0394-4c4c-90bd-1112c10ff062"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[15]: [Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0)),\n Row(a=3, b=4.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 1, 3, 12, 0))]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[15]: [Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0)),\n Row(a=3, b=4.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 1, 3, 12, 0))]"]}}],"execution_count":0},{"cell_type":"code","source":["#avoid throwing an out-of-memory exception\ndf.take(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4f5374d-9040-4763-b605-ea21a1d1f742"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[16]: [Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0))]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[16]: [Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0))]"]}}],"execution_count":0},{"cell_type":"code","source":["df.toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"pandas DataFrame to pandas API","showTitle":true,"inputWidgets":{},"nuid":"940d291f-3230-4776-b352-23863ba37d59"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n      <th>c</th>\n      <th>d</th>\n      <th>e</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2.0</td>\n      <td>string1</td>\n      <td>2000-01-01</td>\n      <td>2000-01-01 12:00:00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>3.0</td>\n      <td>string2</td>\n      <td>2000-02-01</td>\n      <td>2000-01-02 12:00:00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>4.0</td>\n      <td>string3</td>\n      <td>2000-03-01</td>\n      <td>2000-01-03 12:00:00</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n      <th>c</th>\n      <th>d</th>\n      <th>e</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2.0</td>\n      <td>string1</td>\n      <td>2000-01-01</td>\n      <td>2000-01-01 12:00:00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>3.0</td>\n      <td>string2</td>\n      <td>2000-02-01</td>\n      <td>2000-01-02 12:00:00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>4.0</td>\n      <td>string3</td>\n      <td>2000-03-01</td>\n      <td>2000-01-03 12:00:00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.a"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Selecting and Accessing Data","showTitle":true,"inputWidgets":{},"nuid":"7a74f75d-3973-4364-9980-8b6589648aae"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[18]: Column<'a'>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[18]: Column<'a'>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import Column\nfrom pyspark.sql.functions import upper\n\ntype(df.c) == type(upper(df.c)) == type(df.c.isNull())\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"column-wise operations return Columns","showTitle":true,"inputWidgets":{},"nuid":"0e44fe62-b42d-494f-834e-b5ba1fdd5c17"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[20]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[20]: True"]}}],"execution_count":0},{"cell_type":"code","source":[" df.select(df.c).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"selecting a column from dataset","showTitle":true,"inputWidgets":{},"nuid":"2901054a-971a-47bb-8e2d-5724cecd51df"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+\n|      c|\n+-------+\n|string1|\n|string2|\n|string3|\n+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+\n|      c|\n+-------+\n|string1|\n|string2|\n|string3|\n+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df.withColumn('upper_c', upper(df.c)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Assign a new Column ","showTitle":true,"inputWidgets":{},"nuid":"f49f7bac-08c6-4612-a027-98734939f0fb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+---+-------+----------+-------------------+-------+\n|  a|  b|      c|         d|                  e|upper_c|\n+---+---+-------+----------+-------------------+-------+\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|STRING1|\n|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|STRING2|\n|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|STRING3|\n+---+---+-------+----------+-------------------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+---+-------+----------+-------------------+-------+\n|  a|  b|      c|         d|                  e|upper_c|\n+---+---+-------+----------+-------------------+-------+\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|STRING1|\n|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|STRING2|\n|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|STRING3|\n+---+---+-------+----------+-------------------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df.filter(df.a == 1).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"select a subset of rows","showTitle":true,"inputWidgets":{},"nuid":"800ba6bb-8c07-4af5-b3a0-7e86d2516e8c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+---+-------+----------+-------------------+\n|  a|  b|      c|         d|                  e|\n+---+---+-------+----------+-------------------+\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n+---+---+-------+----------+-------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+---+-------+----------+-------------------+\n|  a|  b|      c|         d|                  e|\n+---+---+-------+----------+-------------------+\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n+---+---+-------+----------+-------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\nfrom pyspark.sql.functions import pandas_udf\n\n@pandas_udf('long')\ndef pandas_plus_one(series: pd.Series) -> pd.Series:\n# Simply plus one by using the pandas Series.\n    return series + 1\ndf.select(pandas_plus_one(df.a)).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Pandas UDFs and Pandas Function APIs  pandas Series(Applying a Function)","showTitle":true,"inputWidgets":{},"nuid":"a738a8fe-6338-426c-8fb7-e5fc56bec4e6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------------------+\n|pandas_plus_one(a)|\n+------------------+\n|                 2|\n|                 3|\n|                 4|\n+------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------------+\n|pandas_plus_one(a)|\n+------------------+\n|                 2|\n|                 3|\n|                 4|\n+------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["def pandas_filter_func(iterator):\n       for pandas_df in iterator:\n           yield pandas_df[pandas_df.a == 1]\n\ndf.mapInPandas(pandas_filter_func, schema=df.schema).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":" directly use the APIs in a pandas DataFrame by users","showTitle":true,"inputWidgets":{},"nuid":"6ca525d7-c3d0-4b0d-bcc7-c5322d917b77"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+---+-------+----------+-------------------+\n|  a|  b|      c|         d|                  e|\n+---+---+-------+----------+-------------------+\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n+---+---+-------+----------+-------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+---+-------+----------+-------------------+\n|  a|  b|      c|         d|                  e|\n+---+---+-------+----------+-------------------+\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n+---+---+-------+----------+-------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.createDataFrame([\n    ['red', 'banana', 1, 10], \n    ['blue', 'banana', 2, 20],\n    ['red', 'carrot', 3, 30],\n    ['blue', 'grape', 4, 40], \n    ['red', 'carrot', 5, 50], \n    ['black', 'carrot', 6, 60],\n    ['red', 'banana', 7, 70], \n    ['red', 'grape', 8, 80]], \n    schema=['color', 'fruit', 'v1', 'v2'])\ndf.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Grouping Data(combines them back to the DataFrame)","showTitle":true,"inputWidgets":{},"nuid":"d307a085-83c8-4fa5-a7ae-20d7703bcc53"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+------+---+---+\n|color| fruit| v1| v2|\n+-----+------+---+---+\n|  red|banana|  1| 10|\n| blue|banana|  2| 20|\n|  red|carrot|  3| 30|\n| blue| grape|  4| 40|\n|  red|carrot|  5| 50|\n|black|carrot|  6| 60|\n|  red|banana|  7| 70|\n|  red| grape|  8| 80|\n+-----+------+---+---+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+------+---+---+\n|color| fruit| v1| v2|\n+-----+------+---+---+\n|  red|banana|  1| 10|\n| blue|banana|  2| 20|\n|  red|carrot|  3| 30|\n| blue| grape|  4| 40|\n|  red|carrot|  5| 50|\n|black|carrot|  6| 60|\n|  red|banana|  7| 70|\n|  red| grape|  8| 80|\n+-----+------+---+---+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df.groupby('color').avg().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Grouping and then applying the avg() function ","showTitle":true,"inputWidgets":{},"nuid":"b34e7498-0e57-4e14-84f3-0b2ebf1c8448"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+-------+-------+\n|color|avg(v1)|avg(v2)|\n+-----+-------+-------+\n|  red|    4.8|   48.0|\n| blue|    3.0|   30.0|\n|black|    6.0|   60.0|\n+-----+-------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+-------+-------+\n|color|avg(v1)|avg(v2)|\n+-----+-------+-------+\n|  red|    4.8|   48.0|\n| blue|    3.0|   30.0|\n|black|    6.0|   60.0|\n+-----+-------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["def plus_mean(pandas_df):\n    return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())\n\ndf.groupby('color').applyInPandas(plus_mean, schema=df.schema).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"apply a Python native function against each group by using pandas API","showTitle":true,"inputWidgets":{},"nuid":"333f55e5-3a65-4cb4-b432-bb10e5d0184e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+------+---+---+\n|color| fruit| v1| v2|\n+-----+------+---+---+\n|black|carrot|  0| 60|\n| blue|banana| -1| 20|\n| blue| grape|  1| 40|\n|  red|banana| -3| 10|\n|  red|carrot| -1| 30|\n|  red|carrot|  0| 50|\n|  red|banana|  2| 70|\n|  red| grape|  3| 80|\n+-----+------+---+---+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+------+---+---+\n|color| fruit| v1| v2|\n+-----+------+---+---+\n|black|carrot|  0| 60|\n| blue|banana| -1| 20|\n| blue| grape|  1| 40|\n|  red|banana| -3| 10|\n|  red|carrot| -1| 30|\n|  red|carrot|  0| 50|\n|  red|banana|  2| 70|\n|  red| grape|  3| 80|\n+-----+------+---+---+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df1 = spark.createDataFrame(\n    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n    ('time', 'id', 'v1'))\n\ndf2 = spark.createDataFrame(\n    [(20000101, 1, 'x'), (20000101, 2, 'y')],\n    ('time', 'id', 'v2'))\n\ndef asof_join(l, r):\n    return pd.merge_asof(l, r, on='time', by='id')\n\ndf1.groupby('id').cogroup(df2.groupby('id')).applyInPandas(\n    asof_join, schema='time int, id int, v1 double, v2 string').show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Co-grouping and applying a function.","showTitle":true,"inputWidgets":{},"nuid":"418836b7-4cfc-441c-8d89-d5e32a05ccad"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+---+---+---+\n|    time| id| v1| v2|\n+--------+---+---+---+\n|20000101|  1|1.0|  x|\n|20000102|  1|3.0|  x|\n|20000101|  2|2.0|  y|\n|20000102|  2|4.0|  y|\n+--------+---+---+---+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+---+---+---+\n|    time| id| v1| v2|\n+--------+---+---+---+\n|20000101|  1|1.0|  x|\n|20000102|  1|3.0|  x|\n|20000101|  2|2.0|  y|\n|20000102|  2|4.0|  y|\n+--------+---+---+---+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df.write.csv('foo.csv', header=True)\nspark.read.csv('foo.csv', header=True).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Getting Data in/out","showTitle":true,"inputWidgets":{},"nuid":"e657ce6a-639d-4903-926d-0809f16aeff6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n\u001B[0;32m<command-4052307809996924>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m#df.write.csv('foo.csv', header=True)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'foo.csv'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mcsv\u001B[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001B[0m\n\u001B[1;32m    408\u001B[0m             \u001B[0mpath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    409\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 410\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoSeq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    411\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRDD\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    412\u001B[0m             \u001B[0;32mdef\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: Path must be absolute: foo.csv","errorSummary":"<span class='ansi-red-fg'>IllegalArgumentException</span>: Path must be absolute: foo.csv","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n\u001B[0;32m<command-4052307809996924>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m#df.write.csv('foo.csv', header=True)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'foo.csv'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mcsv\u001B[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001B[0m\n\u001B[1;32m    408\u001B[0m             \u001B[0mpath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    409\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 410\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoSeq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    411\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRDD\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    412\u001B[0m             \u001B[0;32mdef\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: Path must be absolute: foo.csv"]}}],"execution_count":0},{"cell_type":"code","source":["df.write.parquet('bar.parquet')\nspark.read.parquet('bar.parquet').show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Parquet","showTitle":true,"inputWidgets":{},"nuid":"2b73e478-a815-42ba-bbd8-430e08a4b828"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n\u001B[0;32m<command-4052307809996925>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'bar.parquet'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'bar.parquet'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mparquet\u001B[0;34m(self, *paths, **options)\u001B[0m\n\u001B[1;32m    299\u001B[0m                        int96RebaseMode=int96RebaseMode)\n\u001B[1;32m    300\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 301\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_to_seq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpaths\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    303\u001B[0m     def text(self, paths, wholetext=False, lineSep=None, pathGlobFilter=None,\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: Path must be absolute: bar.parquet","errorSummary":"<span class='ansi-red-fg'>IllegalArgumentException</span>: Path must be absolute: bar.parquet","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n\u001B[0;32m<command-4052307809996925>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'bar.parquet'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'bar.parquet'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mparquet\u001B[0;34m(self, *paths, **options)\u001B[0m\n\u001B[1;32m    299\u001B[0m                        int96RebaseMode=int96RebaseMode)\n\u001B[1;32m    300\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 301\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_to_seq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpaths\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    303\u001B[0m     def text(self, paths, wholetext=False, lineSep=None, pathGlobFilter=None,\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: Path must be absolute: bar.parquet"]}}],"execution_count":0},{"cell_type":"code","source":["df.write.orc('zoo.orc')\nspark.read.orc('zoo.orc').show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"ORC","showTitle":true,"inputWidgets":{},"nuid":"c70fb697-cd30-4ffb-ae86-75353d403ce6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n\u001B[0;32m<command-4052307809996926>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'zoo.orc'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'zoo.orc'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36morc\u001B[0;34m(self, path, mergeSchema, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter)\u001B[0m\n\u001B[1;32m    461\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    462\u001B[0m             \u001B[0mpath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 463\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_to_seq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    464\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    465\u001B[0m     def jdbc(self, url, table, column=None, lowerBound=None, upperBound=None, numPartitions=None,\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: Path must be absolute: zoo.orc","errorSummary":"<span class='ansi-red-fg'>IllegalArgumentException</span>: Path must be absolute: zoo.orc","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n\u001B[0;32m<command-4052307809996926>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'zoo.orc'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'zoo.orc'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36morc\u001B[0;34m(self, path, mergeSchema, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter)\u001B[0m\n\u001B[1;32m    461\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    462\u001B[0m             \u001B[0mpath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 463\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_to_seq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    464\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    465\u001B[0m     def jdbc(self, url, table, column=None, lowerBound=None, upperBound=None, numPartitions=None,\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: Path must be absolute: zoo.orc"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Using Spark Dataset,DataFrame in pyspark and scala","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":712757888028650}},"nbformat":4,"nbformat_minor":0}
